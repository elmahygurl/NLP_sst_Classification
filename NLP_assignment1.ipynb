{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elmahygurl/NLP_sst_Classification/blob/main/NLP_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhv37jNcClyn"
      },
      "source": [
        "# Getting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne9toyr5CkLp",
        "outputId": "5773837a-be24-4320-b52d-c64bc96d0e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3D5D_RoDAhW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NdKuRUsDJw7",
        "outputId": "81940953-3473-41bc-f841-02098a8e6294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sst_dataset = load_dataset('sst')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xAHI1AxDKqm",
        "outputId": "11c29fe2-dfe9-4c3a-bbab-e416a5d1932c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'This story still seems timely and important .', 'label': 0.6944400072097778, 'tokens': 'This|story|still|seems|timely|and|important|.', 'tree': '14|14|13|11|9|9|10|12|10|11|12|13|15|15|0'}\n",
            "Number of training examples: 8544\n",
            "Number of validation examples: 1101\n",
            "Number of test examples: 2210\n"
          ]
        }
      ],
      "source": [
        "print(sst_dataset['train'][500])\n",
        "print(f\"Number of training examples: {len(sst_dataset['train'])}\")\n",
        "print(f\"Number of validation examples: {len(sst_dataset['validation'])}\")\n",
        "print(f\"Number of test examples: {len(sst_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IHBlanSB3eu"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6rfAZ8_TfEK"
      },
      "source": [
        "Removing the 'tree' column and adjusting the labels to accomodate the 5 classes we want to classify to where:\n",
        "From 0 to 0.2 (0.2 included) will be class 0 “very negative”.\n",
        "\n",
        "From 0.2 to 0.4 (0.4 included) will be class 1 “negative”.\n",
        "\n",
        "From 0.4 to 0.6 (0.6 included) will be class 2 “neutral”.\n",
        "\n",
        "From 0.6 to 0.8 (0.8 included) will be class 3 “positive”.\n",
        "\n",
        "From 0.8 to 1.0 (1.0 included) will be class 4 “very positive”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5EPCqgimpFE",
        "outputId": "aadefb3b-e81d-4f11-c8b5-f43ea8bc6f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 8544\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 1101\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 2210\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "#converting splits to Pandas DataFrame for manipulation\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train'])\n",
        "test_df = pd.DataFrame(sst_dataset['test'])\n",
        "val_df = pd.DataFrame(sst_dataset['validation'])\n",
        "\n",
        "#function to map float labels to our desired categorical classes\n",
        "def map_labels_to_categories(label):\n",
        "    if 0.0 <= label <= 0.2:\n",
        "        return 0  # very negative\n",
        "    elif 0.2 < label <= 0.4:\n",
        "        return 1  # negative\n",
        "    elif 0.4 < label <= 0.6:\n",
        "        return 2  # neutral\n",
        "    elif 0.6 < label <= 0.8:\n",
        "        return 3  # positive\n",
        "    elif 0.8 < label <= 1.0:\n",
        "        return 4  # very positive\n",
        "\n",
        "#mapping function to create a new column with mapped labels\n",
        "train_df['mapped_label'] = train_df['label'].apply(map_labels_to_categories)\n",
        "test_df['mapped_label'] = test_df['label'].apply(map_labels_to_categories)\n",
        "val_df['mapped_label'] = val_df['label'].apply(map_labels_to_categories)\n",
        "\n",
        "#dropping the original label column\n",
        "train_df = train_df.drop('label', axis=1)\n",
        "test_df = test_df.drop('label', axis=1)\n",
        "val_df = val_df.drop('label', axis=1)\n",
        "\n",
        "train_df = train_df.drop(columns=['tree']) #dropping 'tree' column\n",
        "test_df = test_df.drop(columns=['tree'])\n",
        "val_df = val_df.drop(columns=['tree'])\n",
        "\n",
        "######just to print an output and visualise\n",
        "#convert the DataFrame back to the datasets format\n",
        "sst_dataset['train'] = Dataset.from_pandas(train_df)\n",
        "sst_dataset['test'] = Dataset.from_pandas(test_df)\n",
        "sst_dataset['validation'] = Dataset.from_pandas(val_df)\n",
        "\n",
        "#display the updated dataset\n",
        "print(sst_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688DssCrsigk",
        "outputId": "1d1e8f7c-fa48-4a01-f968-58276c0e5bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Split Label Distribution:\n",
            "0    1092\n",
            "1    2218\n",
            "2    1624\n",
            "3    2322\n",
            "4    1288\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Test Split Label Distribution:\n",
            "0    279\n",
            "1    633\n",
            "2    389\n",
            "3    510\n",
            "4    399\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Validation Split Label Distribution:\n",
            "0    139\n",
            "1    289\n",
            "2    229\n",
            "3    279\n",
            "4    165\n",
            "Name: mapped_label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#analyzing the distribution of labels\n",
        "train_label_distribution = train_df['mapped_label'].value_counts().sort_index()\n",
        "test_label_distribution = test_df['mapped_label'].value_counts().sort_index()\n",
        "val_label_distribution = val_df['mapped_label'].value_counts().sort_index()\n",
        "\n",
        "print(\"Training Split Label Distribution:\")\n",
        "print(train_label_distribution)\n",
        "\n",
        "print(\"\\nTest Split Label Distribution:\")\n",
        "print(test_label_distribution)\n",
        "\n",
        "print(\"\\nValidation Split Label Distribution:\")\n",
        "print(val_label_distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbDu1oUutxsT"
      },
      "source": [
        "### Visualisation for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "5ZmT1vVBd1Fy",
        "outputId": "7f94af34-a9fc-4192-c636-e6ac086903fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train']) #convert the 'train' split to a Pandas DataFrame\n",
        "\n",
        "# Plot the distribution of mapped labels in the 'train' split\n",
        "train_df['mapped_label'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Mapped Labels in the Train Split')\n",
        "plt.xlabel('Mapped Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_iFiVO7TAe6",
        "outputId": "29b1bde9-370f-4535-f223-2eb7985e098d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  \\\n",
            "0  The Rock is destined to be the 21st Century 's...   \n",
            "1  The gorgeously elaborate continuation of `` Th...   \n",
            "2  Singer\\/composer Bryan Adams contributes a sle...   \n",
            "3  You 'd think by now America would have had eno...   \n",
            "4               Yet the act is still charming here .   \n",
            "\n",
            "                                              tokens  mapped_label  \n",
            "0  The|Rock|is|destined|to|be|the|21st|Century|'s...             3  \n",
            "1  The|gorgeously|elaborate|continuation|of|``|Th...             4  \n",
            "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...             3  \n",
            "3  You|'d|think|by|now|America|would|have|had|eno...             2  \n",
            "4               Yet|the|act|is|still|charming|here|.             3  \n",
            "Unique values in mapped_labels =  [3 4 2 1 0]\n",
            "Missing =  0\n"
          ]
        }
      ],
      "source": [
        "#print first few rows of the 'train' split to see the transformed data\n",
        "print(train_df.head())\n",
        "\n",
        "#to ensure correct mapping\n",
        "print(\"Unique values in mapped_labels = \",train_df['mapped_label'].unique())\n",
        "\n",
        "#check for missing values in the 'mapped_label' column of the 'train' split\n",
        "print(\"Missing = \",train_df['mapped_label'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngi3yEz6q7rb"
      },
      "source": [
        "## Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FMn6E9nb3bk",
        "outputId": "f58f99c7-89d6-4e29-eae3-4496ac963409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when splitting sentence: 0.39683257918552034\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def train_naive_bayes(D, C):\n",
        "    Ndoc = len(D)   #total number of documents in training dataset D\n",
        "    Nc = np.array([np.sum(D['mapped_label'] == c) for c in C]) #calculates no. of documents in D that belong to each class in C\n",
        "    logprior = np.log(Nc / Ndoc) #calculating prior probability of each class\n",
        "\n",
        "    #building vocabulary of D\n",
        "    V = set()\n",
        "    for doc in D['sentence']:\n",
        "        V.update(doc.split())  #adding to V so it has unique words\n",
        "    V = list(V) #converting to list\n",
        "\n",
        "    #calculating P(w|c) terms\n",
        "    bigdoc = {}  #new dictionary\n",
        "    for c in C:\n",
        "        bigdoc[c] = ' '.join(D[D['mapped_label'] == c]['sentence']).split() #concatenate sentences of documents belonging to each class and split them into words\n",
        "\n",
        "    #calculating frequency of each word in vocabulary for each class and store counts in a 2D NumPy array count\n",
        "    count = np.zeros((len(V), len(C)))\n",
        "    for i, word in enumerate(V):\n",
        "        for j, c in enumerate(C):\n",
        "            count[i, j] = bigdoc[c].count(word)\n",
        "\n",
        "    #calculate log likelihoods of each word given each class\n",
        "    #using Laplace smoothing to avoid zero probabilities\n",
        "    loglikelihood = np.log((count + 1) / (np.sum(count, axis=0, keepdims=True) + len(V)))\n",
        "\n",
        "    return logprior, loglikelihood, V\n",
        "\n",
        "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
        "    sum_ = np.zeros(len(C))  # to store log probabilities of each class for the given testdoc\n",
        "    for c in C:\n",
        "        sum_[c] = logprior[c]\n",
        "        for word in testdoc.split():\n",
        "            if word in V:  #if present in vocab\n",
        "                sum_[c] += loglikelihood[V.index(word), c]\n",
        "\n",
        "    return np.argmax(sum_) #returns index of the class with the highest log probability (indicating predicted class for the testdoc)\n",
        "\n",
        "#list with the classes we have (aka labels)\n",
        "C = [0, 1, 2, 3, 4]\n",
        "\n",
        "#Trainingg\n",
        "logprior, loglikelihood, V = train_naive_bayes(train_df, C)\n",
        "\n",
        "#Testingg\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "for i in range(total):\n",
        "    predicted_label = test_naive_bayes(test_df.iloc[i]['sentence'], logprior, loglikelihood, C, V)\n",
        "    if predicted_label == test_df.iloc[i]['mapped_label']:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy when splitting sentence:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XmaIeNrO4ex"
      },
      "source": [
        "#####same thing but with existing tokens in dataset not spliting the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0clLFYYS99F-",
        "outputId": "ee416e29-f959-4d8a-c922-978e3307a5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with existing tokens in dataset: 0.2864253393665158\n"
          ]
        }
      ],
      "source": [
        "def train_naive_bayes1(D, C):\n",
        "    #calculate P(c) terms\n",
        "    Ndoc = len(D)\n",
        "    Nc = np.array([np.sum(D['mapped_label'] == c) for c in C])\n",
        "    logprior = np.log(Nc / Ndoc)\n",
        "\n",
        "    # building vocab with existing tokens in dataset\n",
        "    V = set()\n",
        "    for tokens in D['tokens']:\n",
        "        V.update(tokens)\n",
        "    V = list(V)\n",
        "\n",
        "    #calculate P(w|c) terms\n",
        "    bigdoc = {}\n",
        "    for c in C:\n",
        "        bigdoc[c] = [token for tokens in D[D['mapped_label'] == c]['tokens'] for token in tokens]\n",
        "\n",
        "    count = np.zeros((len(V), len(C)))\n",
        "    for i, word in enumerate(V):\n",
        "        for j, c in enumerate(C):\n",
        "            count[i, j] = bigdoc[c].count(word)\n",
        "\n",
        "    loglikelihood = np.log((count + 1) / (np.sum(count, axis=0, keepdims=True) + len(V)))\n",
        "\n",
        "    return logprior, loglikelihood, V\n",
        "\n",
        "def test_naive_bayes1(testdoc, logprior, loglikelihood, C, V):\n",
        "    sum_ = np.zeros(len(C))\n",
        "    for c in C:\n",
        "        sum_[c] = logprior[c]\n",
        "        for word in testdoc:\n",
        "            if word in V:\n",
        "                sum_[c] += loglikelihood[V.index(word), c]\n",
        "\n",
        "    return np.argmax(sum_)\n",
        "\n",
        "#classes\n",
        "C = [0, 1, 2, 3, 4]\n",
        "\n",
        "logprior, loglikelihood, V = train_naive_bayes1(train_df, C)\n",
        "\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "for i in range(total):\n",
        "    predicted_label = test_naive_bayes1(test_df.iloc[i]['tokens'], logprior, loglikelihood, C, V)\n",
        "    if predicted_label == test_df.iloc[i]['mapped_label']:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy with existing tokens in dataset:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qUnpJ67QAaE"
      },
      "source": [
        "####Metrics and comparison with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "sf6IBPCXvRuX",
        "outputId": "0ce32219-0120-4924-d07e-796487c2d4ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
              "                ('classifier', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "#create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB(alpha=1.0))  # Alpha parameter for Laplace smoothing\n",
        "    ])\n",
        "\n",
        "#Train pipeline\n",
        "pipeline.fit(train_df['sentence'], train_df['mapped_label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB57jC_bPsrJ",
        "outputId": "e0525ee6-2f1b-426e-ddf2-60010e7bcabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (scikit-learn): 0.4090497737556561\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "predictions_sklearn = pipeline.predict(test_df['sentence'])\n",
        "\n",
        "accuracy_sklearn = accuracy_score(test_df['mapped_label'], predictions_sklearn)\n",
        "print(\"Accuracy (scikit-learn):\", accuracy_sklearn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCgCXEZCSbMk",
        "outputId": "7bf66140-9be0-43c7-8f08-b6eb24ce7f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (only using numpy):\n",
            " [[ 28 200  10  41   0]\n",
            " [ 22 415  57 127  12]\n",
            " [  8 154  34 181  12]\n",
            " [  2  88  39 340  41]\n",
            " [  2  39  12 259  87]]\n",
            "Precision: [0.4516129  0.46316964 0.22368421 0.35864979 0.57236842]\n",
            "Recall: [0.10035842 0.65560821 0.0874036  0.66666667 0.21804511]\n",
            "F1 Score: [0.16422287 0.54283846 0.12569316 0.46639232 0.31578947]\n",
            "Macro-averaged Precision: 0.41389699333828645\n",
            "Macro-averaged Recall: 0.3456164032418666\n",
            "Macro-averaged F1 Score: 0.3229872566299008\n"
          ]
        }
      ],
      "source": [
        "#  function to make the Confusion Matrix\n",
        "def confusion_matrix_numpy(y_true, y_pred, num_classes):\n",
        "    confusion_mat = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        confusion_mat[true][pred] += 1\n",
        "    return confusion_mat\n",
        "\n",
        "conf_matrix_numpy = confusion_matrix_numpy(test_df['mapped_label'], predictions_sklearn, len(C))\n",
        "print(\"Confusion Matrix (only using numpy):\\n\", conf_matrix_numpy)\n",
        "\n",
        "# Precision, Recall, and F1 score per class\n",
        "def precision_recall_f1_numpy(conf_matrix):\n",
        "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
        "    recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "precision_numpy, recall_numpy, f1_numpy = precision_recall_f1_numpy(conf_matrix_numpy)\n",
        "print(\"Precision:\", precision_numpy)\n",
        "print(\"Recall:\", recall_numpy)\n",
        "print(\"F1 Score:\", f1_numpy)\n",
        "\n",
        "# Macro-averaged Precision, Recall, and F1 score\n",
        "def macro_averaged_metrics_numpy(precision, recall, f1):\n",
        "    macro_precision = np.mean(precision)\n",
        "    macro_recall = np.mean(recall)\n",
        "    macro_f1 = np.mean(f1)\n",
        "    return macro_precision, macro_recall, macro_f1\n",
        "\n",
        "precision_macro_numpy, recall_macro_numpy, f1_macro_numpy = macro_averaged_metrics_numpy(precision_numpy, recall_numpy, f1_numpy)\n",
        "print(\"Macro-averaged Precision:\", precision_macro_numpy)\n",
        "print(\"Macro-averaged Recall:\", recall_macro_numpy)\n",
        "print(\"Macro-averaged F1 Score:\", f1_macro_numpy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XGpXx7mQ4MU"
      },
      "source": [
        "##### better representation (for us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHTx14dDP7NS",
        "outputId": "882470c5-1e2e-4924-e6e1-1dc618d67a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics per class (scikit-learn):\n",
            "    Precision    Recall  F1 Score\n",
            "0   0.451613  0.100358  0.164223\n",
            "1   0.463170  0.655608  0.542838\n",
            "2   0.223684  0.087404  0.125693\n",
            "3   0.358650  0.666667  0.466392\n",
            "4   0.572368  0.218045  0.315789\n",
            "-------------------------------------\n",
            "Macro-averaged Metrics (scikit-learn):\n",
            "    Precision    Recall  F1 Score\n",
            "0   0.413897  0.345616  0.322987\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#create DataFrame for precision, recall, and F1 score\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_numpy,\n",
        "    'Recall': recall_numpy,\n",
        "    'F1 Score': f1_numpy\n",
        "}, index=C)  # Assuming C contains the class labels\n",
        "\n",
        "\n",
        "# create DataFrame for macro-averaged precision, recall, and F1 score\n",
        "macro_metrics_df = pd.DataFrame({\n",
        "    'Precision': [precision_macro_numpy],\n",
        "    'Recall': [recall_macro_numpy],\n",
        "    'F1 Score': [f1_macro_numpy]\n",
        "})\n",
        "\n",
        "print(\"Metrics per class (scikit-learn):\\n\", metrics_df)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Macro-averaged Metrics (scikit-learn):\\n\", macro_metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSRDo1llyxWX"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB2JtwtH8Orc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume train_data is your Dataset object containing 'sentence', 'tokens', and 'mapped_label'\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "sentences = train_df['sentence']\n",
        "tokens = train_df['tokens']\n",
        "mapped_labels = train_df['mapped_label']\n",
        "\n",
        "word_bi_grams = []\n",
        "for sentence_tokens in tokens:\n",
        "    bi_grams = [(sentence_tokens[i], sentence_tokens[i + 1]) for i in range(len(sentence_tokens) - 1)]\n",
        "    word_bi_grams.extend(bi_grams)\n",
        "\n",
        "\n",
        "vocab = list(set(word_bi_grams))\n",
        "vocab_size = len(vocab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIMtnPMy8fS0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Feature Representation\n",
        "def create_feature_vector(sentence_tokens, vocab):\n",
        "    feature_vector = np.zeros(len(vocab))\n",
        "    for i in range(len(sentence_tokens) - 1):\n",
        "        bi_gram = (sentence_tokens[i], sentence_tokens[i + 1])\n",
        "        if bi_gram in vocab:\n",
        "            feature_vector[vocab.index(bi_gram)] = 1\n",
        "    return feature_vector\n",
        "\n",
        "X_train = np.array([create_feature_vector(tokens[i], vocab) for i in range(len(tokens))])\n",
        "y_train = np.array(mapped_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_GBpk7P8mDg"
      },
      "source": [
        "BiGramClass:\n",
        "\n",
        "```\n",
        " we implimented the three steps in one class to help us during the testing phase\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwHQ4r5Yo-yh",
        "outputId": "3be22b2f-25fa-40b4-f002-81e1f3522c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: -0.4226113239589358\n",
            "Epoch 10, Loss: -0.9729861013608493\n",
            "Epoch 20, Loss: -1.506480684175596\n",
            "Epoch 30, Loss: -2.0239870093523002\n",
            "Epoch 40, Loss: -2.526357977107247\n",
            "Epoch 50, Loss: -3.0144021841827398\n",
            "Epoch 60, Loss: -3.4888976355773913\n",
            "Epoch 70, Loss: -3.950610609821099\n",
            "Epoch 80, Loss: -4.400303159778472\n",
            "Epoch 90, Loss: -4.838722214332863\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "train_dataset = sst_dataset['train']\n",
        "class BiGramLogisticRegression:\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, num_epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.vocab = None\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def initialize_params(self, num_features):\n",
        "        self.weights = np.random.randn(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-10\n",
        "        return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "\n",
        "#the preprocessing bi-gram set\n",
        "    def preprocess_and_create_vocab(self, data):\n",
        "        sentences = data['sentence']\n",
        "        tokens = data['tokens']\n",
        "\n",
        "        word_bi_grams = []\n",
        "        for sentence_tokens in tokens:\n",
        "            bi_grams = [(sentence_tokens[i], sentence_tokens[i + 1]) for i in range(len(sentence_tokens) - 1)]\n",
        "            word_bi_grams.extend(bi_grams)\n",
        "\n",
        "        self.vocab = list(set(word_bi_grams))\n",
        "\n",
        "#creating a vector of 1 if bigram exists in dataset else 0\n",
        "    def create_feature_vector(self, sentence_tokens):\n",
        "        feature_vector = np.zeros(len(self.vocab))\n",
        "        for i in range(len(sentence_tokens) - 1):\n",
        "            bi_gram = (sentence_tokens[i], sentence_tokens[i + 1])\n",
        "            if bi_gram in self.vocab:\n",
        "                feature_vector[self.vocab.index(bi_gram)] = 1\n",
        "        return feature_vector\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.preprocess_and_create_vocab(data)\n",
        "        X_train = np.array([self.create_feature_vector(tokens) for tokens in data['tokens']])\n",
        "        y_train = np.array(data['mapped_label'])\n",
        "\n",
        "        num_samples, num_features = X_train.shape\n",
        "        self.initialize_params(num_features)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Forward pass\n",
        "            logits = np.dot(X_train, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(logits)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(y_train, y_pred)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / num_samples) * np.dot(X_train.T, (y_pred - y_train))\n",
        "            db = (1 / num_samples) * np.sum(y_pred - y_train)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, data):\n",
        "        X_test = np.array([self.create_feature_vector(tokens) for tokens in data['tokens']])\n",
        "        logits = np.dot(X_test, self.weights) + self.bias\n",
        "        y_pred = self.sigmoid(logits)\n",
        "\n",
        "        # Define class boundaries\n",
        "        thresholds = [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "        # Classify based on thresholds\n",
        "        predictions = np.zeros(len(y_pred), dtype=int)\n",
        "        for i, pred in enumerate(y_pred):\n",
        "            if pred < thresholds[0]:\n",
        "                predictions[i] = 0\n",
        "            elif pred < thresholds[1]:\n",
        "                predictions[i] = 1\n",
        "            elif pred < thresholds[2]:\n",
        "                predictions[i] = 2\n",
        "            elif pred < thresholds[3]:\n",
        "                predictions[i] = 3\n",
        "            else:\n",
        "                predictions[i] = 4  # For values >= 0.8\n",
        "        return predictions\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = BiGramLogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLxyyXkvwV8Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_predictions= model.predict(sst_dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUj8GhUMnDOR",
        "outputId": "2740cd6d-de07-47de-8bfd-694153312696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 52.  17.  11.  22. 177.]\n",
            " [123.  25.  26.  28. 431.]\n",
            " [ 60.  21.  19.  23. 266.]\n",
            " [ 67.  17.  26.  20. 380.]\n",
            " [ 48.  13.  12.  19. 307.]]\n"
          ]
        }
      ],
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "    num_classes = len(np.unique(y_true))\n",
        "    matrix = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        true_label = y_true[i]\n",
        "        predicted_label = y_pred[i]\n",
        "        matrix[true_label][predicted_label] += 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(sst_dataset['test']['mapped_label'], test_predictions)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qCbqV7sriCt",
        "outputId": "149bf308-7761-4572-b126-c2f899c19f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision: [0.14857143 0.2688172  0.20212766 0.17857143 0.1966688 ]\n",
            "Recall: [0.18637993 0.03949447 0.04884319 0.03921569 0.76942356]\n",
            "Accuracy: 0.19140271493212668\n",
            "F1 Score: [0.16534181 0.06887052 0.07867495 0.06430868 0.31326531]\n"
          ]
        }
      ],
      "source": [
        "def precision(conf_matrix):\n",
        "    true_positives = np.diag(conf_matrix)\n",
        "    false_positives = np.sum(conf_matrix, axis=0) - true_positives\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        precision_values = true_positives / (true_positives + false_positives)\n",
        "        precision_values[np.isnan(precision_values)] = 0  # Replace NaN values with zero\n",
        "    return precision_values\n",
        "\n",
        "def recall(conf_matrix):\n",
        "    true_positives = np.diag(conf_matrix)\n",
        "    false_negatives = np.sum(conf_matrix, axis=1) - true_positives\n",
        "    return true_positives / (true_positives + false_negatives)\n",
        "\n",
        "def accuracy(conf_matrix):\n",
        "    correct_predictions = np.sum(np.diag(conf_matrix))\n",
        "    total_predictions = np.sum(conf_matrix)\n",
        "    return correct_predictions / total_predictions\n",
        "\n",
        "def f1_score(precision, recall):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        f1_values = 2 * (precision * recall) / (precision + recall)\n",
        "        f1_values[np.isnan(f1_values)] = 0  # Replace NaN values with zero\n",
        "    return f1_values\n",
        "\n",
        "\n",
        "\n",
        "precisions = precision(conf_matrix)\n",
        "recalls = recall(conf_matrix)\n",
        "accur = accuracy(conf_matrix)\n",
        "f1_scores = f1_score(precisions, recalls)\n",
        "print(\"\\nPrecision:\", precisions)\n",
        "print(\"Recall:\", recalls)\n",
        "print(\"Accuracy:\", accur)\n",
        "print(\"F1 Score:\", f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vQ-E0hN75t8"
      },
      "source": [
        "**SKLEARN**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2joOirZNswyr",
        "outputId": "631bf7e9-d044-4b31-b707-5a4623922729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 36 136  41  58   8]\n",
            " [ 38 332  97 148  18]\n",
            " [ 18 152  57 142  20]\n",
            " [  4 142  49 274  41]\n",
            " [ 11  77  35 192  84]]\n",
            "\n",
            "Precision: [0.3364486  0.39570918 0.20430108 0.33660934 0.49122807]\n",
            "Recall: [0.12903226 0.52448657 0.14652956 0.5372549  0.21052632]\n",
            "Accuracy: 0.35429864253393667\n",
            "F1 Score: [0.1865285  0.45108696 0.17065868 0.41389728 0.29473684]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "test_data=sst_dataset['test']\n",
        "# Preprocess the text data\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))  # Using word bi-grams\n",
        "X_train_vectorized = vectorizer.fit_transform(train_dataset['sentence'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['sentence'])\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_vectorized, train_dataset['mapped_label'])\n",
        "\n",
        "# Predict labels for the test dataset\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix = confusion_matrix(test_data['mapped_label'], y_pred)\n",
        "accuracy = accuracy_score(test_data['mapped_label'], y_pred)\n",
        "precision = precision_score(test_data['mapped_label'], y_pred, average=None)\n",
        "recall = recall_score(test_data['mapped_label'], y_pred, average=None)\n",
        "f1 = f1_score(test_data['mapped_label'], y_pred, average=None)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYNDLTzcwAqz",
        "outputId": "4ddbba95-84c4-453c-c002-127cb89004d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8742081447963801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming train_df is a DataFrame containing 'sentence' and 'mapped_label' columns\n",
        "X_train = train_df['sentence'].values.astype(str)  # Convert to string array\n",
        "y_train = train_df['mapped_label'].values\n",
        "\n",
        "# Assuming sst_dataset is a dictionary containing 'test' dataset\n",
        "X_test = np.array(sst_dataset['test']['sentence']).astype(str)\n",
        "y_test = np.array(sst_dataset['test']['mapped_label'])\n",
        "\n",
        "# Convert continuous values to binary labels\n",
        "y_train = (y_train >= 0.5).astype(int)\n",
        "y_test = (y_test >= 0.5).astype(int)\n",
        "\n",
        "# Create a pipeline with CountVectorizer and SGDClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(ngram_range=(2, 2))),\n",
        "    ('classifier', SGDClassifier(loss='log', max_iter=1000))  # Use SGD with logistic regression\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXyz6q_kCUB8"
      },
      "source": [
        "SECOND TRY:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE2prc0gMJ61"
      },
      "outputs": [],
      "source": [
        "# Step 2: Feature Representation\n",
        "def extract_word_bigrams_with_labels(sentences, labels):\n",
        "    word_bigrams = []\n",
        "    bigram_labels = []\n",
        "\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        words = sentence.split()  # Split sentence into words\n",
        "        sentence_bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "        word_bigrams.extend(sentence_bigrams)\n",
        "        bigram_labels.extend([label] * len(sentence_bigrams))\n",
        "\n",
        "    return word_bigrams, bigram_labels\n",
        "\n",
        "def extract_features(word_bigrams, bigram_labels):\n",
        "    bi_gram_count = {}\n",
        "    unique_bigrams = set()\n",
        "    unique_labels = []\n",
        "\n",
        "    for bigram, label in zip(word_bigrams, bigram_labels):\n",
        "        bigram_lower = (bigram[0].lower(), bigram[1].lower())\n",
        "        if bigram_lower not in bi_gram_count:\n",
        "            count = 0\n",
        "            for sentence_bigram in word_bigrams:\n",
        "                sentence_bigram_lower = (sentence_bigram[0].lower(), sentence_bigram[1].lower())\n",
        "                if bigram_lower == sentence_bigram_lower:\n",
        "                    count += 1\n",
        "            bi_gram_count[bigram_lower] = count\n",
        "\n",
        "            if count == 1:\n",
        "                unique_bigrams.add(bigram)\n",
        "                unique_labels.append(label)\n",
        "\n",
        "    return unique_bigrams, unique_labels\n",
        "\n",
        "def sentence_to_binary_vector(sentence, unique_bigrams):\n",
        "    words = sentence.split()\n",
        "    sentence_bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "    binary_vector = [1 if bigram in sentence_bigrams else 0 for bigram in unique_bigrams]\n",
        "    return binary_vector\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFDte3MUwAq0"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def extract_word_bigrams_with_labels(sentences, labels):\n",
        "    word_bigrams = []\n",
        "    bigram_labels = []\n",
        "\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        words = sentence.split()\n",
        "        sentence_bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "        word_bigrams.extend(sentence_bigrams)\n",
        "        bigram_labels.extend([label] * len(sentence_bigrams))\n",
        "\n",
        "    return word_bigrams, bigram_labels\n",
        "\n",
        "def extract_features(word_bigrams, bigram_labels):\n",
        "    bigram_counter = Counter(word_bigrams)\n",
        "    unique_bigrams = {bigram for bigram, count in bigram_counter.items() if count == 1}\n",
        "    unique_labels = [label for bigram, label in zip(word_bigrams, bigram_labels) if bigram in unique_bigrams]\n",
        "\n",
        "    return unique_bigrams, unique_labels\n",
        "\n",
        "def sentence_to_binary_vector(sentence, unique_bigrams):\n",
        "    words = sentence.split()\n",
        "    sentence_bigrams = {(words[i], words[i+1]) for i in range(len(words)-1)}\n",
        "    binary_vector = [1 if bigram in sentence_bigrams else 0 for bigram in unique_bigrams]\n",
        "    return binary_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObKfeGA2XdeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8211a0a2-4844-451f-eb36-1f4604d32cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2210\n",
            "70416\n",
            "70416\n",
            "(1, 70416)\n",
            "(155619360,)\n",
            "[[3 3 3 ... 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "sentences = sst_dataset['train']['sentence']\n",
        "labels = sst_dataset['train']['mapped_label']\n",
        "\n",
        "word_bigrams, bigram_labels = extract_word_bigrams_with_labels(sentences, labels)\n",
        "unique_bigrams, unique_labels = extract_features(word_bigrams, bigram_labels)\n",
        "testsentences = sst_dataset['test']['sentence']\n",
        "sbinary_vectors = []\n",
        "for sentence in testsentences:\n",
        "    # Convert the sentence into a binary vector\n",
        "    binary_vector = sentence_to_binary_vector(sentence, unique_bigrams)\n",
        "    # Append the binary vector to the list\n",
        "    sbinary_vectors.append(binary_vector)\n",
        "\n",
        "import numpy as np\n",
        "print(len(sbinary_vectors))  # Corrected variable name\n",
        "print(len(sbinary_vectors[0]))\n",
        "\n",
        "print(len(unique_labels))\n",
        "unique_labels_array = np.array(unique_labels)\n",
        "\n",
        "# Reshape the array to be a row\n",
        "unique_labels_row = unique_labels_array.reshape(1, -1)\n",
        "\n",
        "# Printing the shape of the resulting row\n",
        "print(unique_labels_row.shape)\n",
        "concatenated_array = np.concatenate(sbinary_vectors)  # Corrected variable name\n",
        "print(concatenated_array.shape)\n",
        "\n",
        "print(unique_labels_row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_oBgF6-wAq1"
      },
      "outputs": [],
      "source": [
        "testsentences = sst_dataset['test']['sentence']\n",
        "sbinary_vectors = []\n",
        "for sentence in testsentences:\n",
        "    # Convert the sentence into a binary vector\n",
        "    binary_vector = sentence_to_binary_vector(sentence, unique_bigrams)\n",
        "    # Append the binary vector to the list\n",
        "    sbinary_vectors.append(binary_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpDXahHwwAq1",
        "outputId": "982863d9-761a-492a-9720-2c000a189cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8544\n",
            "70416\n",
            "70416\n",
            "(1, 70416)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(len(binary_vectors))\n",
        "print(len(binary_vectors[0]))\n",
        "\n",
        "print(len(unique_labels))\n",
        "unique_labels_array = np.array(unique_labels)\n",
        "\n",
        "# Reshape the array to be a row\n",
        "unique_labels_row = unique_labels_array.reshape(1, -1)\n",
        "\n",
        "# Printing the shape of the resulting row\n",
        "print(unique_labels_row.shape)\n",
        "concatenated_array = np.concatenate(binary_vectors)\n",
        "print(concatenated_array.shape)\n",
        "\n",
        "print(unique_labels_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3we7FF74wAq6",
        "outputId": "b1fc2c7a-52e5-404a-b912-1a6060804231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70416\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(binary_vectors[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGg-oXtiwAq7"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression():\n",
        "  def __init__(self, lr=0.1, n_iter=10):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "  def fit(self, x, y, batch_size=32):\n",
        "    n_samples, n_features = x.shape\n",
        "    n_classes = len(np.unique(y))\n",
        "    self.weights = np.zeros((n_features, n_classes))\n",
        "    self.bias = np.zeros(n_classes)\n",
        "\n",
        "    for _ in range(self.n_iter):\n",
        "        # Shuffle the data\n",
        "        print(_)\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        x_shuffled = x[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            x_batch = x_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "            linear_pred = np.dot(x_batch, self.weights) + self.bias\n",
        "            predict = self._softmax(linear_pred)\n",
        "\n",
        "            dw = (1 / len(x_batch)) * np.dot(x_batch.T, (predict - self._one_hot_encode(y_batch, n_classes)))\n",
        "            db = (1 / len(x_batch)) * np.sum(predict - self._one_hot_encode(y_batch, n_classes), axis=0)\n",
        "\n",
        "            self.weights = self.weights - self.lr * dw\n",
        "            self.bias = self.bias - self.lr * db\n",
        "\n",
        "  def _one_hot_encode(self, labels, num_classes):\n",
        "      one_hot_labels = np.zeros((len(labels), num_classes))\n",
        "      for i, label in enumerate(labels):\n",
        "        one_hot_labels[i, label] = 1\n",
        "      return one_hot_labels\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "        linear_pred = np.dot(x, self.weights) + self.bias\n",
        "        y_pred = self._softmax(linear_pred)\n",
        "        class_pred = np.argmax(y_pred, axis=1)\n",
        "        return class_pred\n",
        "\n",
        "  def _softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return e_x / e_x.sum(axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5e3R9clwAq7"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the LogisticRegression class\n",
        "model = LogisticRegression()\n",
        "# Convert lists to NumPy arrays\n",
        "X_train = np.array(sbinary_vectors)\n",
        "Y_train = np.array(unique_labels)\n",
        "\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmxw5xYFwAq7"
      },
      "outputs": [],
      "source": [
        "predicted= model.predict(sbinary_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qyaGShSwAq7",
        "outputId": "b02b348a-74f0-4423-a0c5-95b8ac846779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]\n",
            " [9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]\n",
            " [9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]\n",
            " ...\n",
            " [9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]\n",
            " [9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]\n",
            " [9.74025448e-06 3.23029206e-10 1.73372546e-05 9.25062205e-01\n",
            "  7.49107170e-02]]\n",
            "[[9.74025448e-06 3.23029206e-10 1.73372546e-05 ... 1.73372546e-05\n",
            "  9.25062205e-01 7.49107170e-02]]\n"
          ]
        }
      ],
      "source": [
        "print(predicted)\n",
        "predicted = np.array(predicted).reshape(1, -1)\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JNd1x3nwAq7",
        "outputId": "a79739e2-58d6-4e4c-ae9e-4aa9d3272655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nadaa\\AppData\\Local\\Temp\\ipykernel_25916\\3264339732.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  correct = np.sum(true_labels == predicted_labels)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def accuracy(true_labels, predicted_labels):\n",
        "    correct = np.sum(true_labels == predicted_labels)\n",
        "    total = len(true_labels)\n",
        "    return correct / total\n",
        "\n",
        "# Example usage:\n",
        "true_labels = sst_dataset['train']['mapped_label']\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "acc = accuracy(true_labels, predicted)\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6Hjj6nRhWhe",
        "colab": {
          "referenced_widgets": [
            "bc8e401d87a4415b9702f0c627887d2b"
          ]
        },
        "outputId": "2b7fc2c4-d19a-495c-c5a7-c0e6463fdcfa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc8e401d87a4415b9702f0c627887d2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[44], line 51\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(y_predicted, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m softmax_model \u001b[38;5;241m=\u001b[39m SoftmaxRegression()\n\u001b[1;32m---> 51\u001b[0m softmax_model\u001b[38;5;241m.\u001b[39mtrain(binary_vectors, unique_labels)\n",
            "Cell \u001b[1;32mIn[44], line 27\u001b[0m, in \u001b[0;36mSoftmaxRegression.train\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations)):\n\u001b[0;32m     26\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)  \u001b[38;5;66;03m# select random batch indices\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X[indices], y[indices]  \u001b[38;5;66;03m# get batches\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     linear_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[0;32m     29\u001b[0m     y_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(linear_model)\n",
            "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=10, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.weights = None\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z))\n",
        "        return e_z / e_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        X = np.array(X) if isinstance(X, list) else X\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # 1. Initialize weights\n",
        "        self.weights = np.zeros((num_features, num_classes))\n",
        "\n",
        "        # 2. Gradient descent loop\n",
        "        for i in tqdm(range(self.num_iterations)):\n",
        "            indices = np.random.choice(num_samples, self.batch_size)  # select random batch indices\n",
        "            X_batch, y_batch = X[indices], y[indices]  # get batches\n",
        "            linear_model = np.dot(X_batch, self.weights)\n",
        "            y_predicted = self.softmax(linear_model)\n",
        "\n",
        "            # 3. Compute gradients\n",
        "            y_actual = np.zeros(y_predicted.shape)\n",
        "            y_actual[np.arange(self.batch_size), y_batch] = 1\n",
        "            gradient = (1 / self.batch_size) * np.dot(X_batch.T, (y_predicted - y_actual))\n",
        "\n",
        "            # 4. Update weights\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # 5. Delete batch from memory\n",
        "            del X_batch, y_batch\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights)\n",
        "        y_predicted = self.softmax(linear_model)\n",
        "        return np.argmax(y_predicted, axis=1)\n",
        "\n",
        "\n",
        "softmax_model = SoftmaxRegression()\n",
        "\n",
        "softmax_model.train(binary_vectors, unique_labels)\n",
        "\n",
        "# # Example predictions:\n",
        "# X_test = np.array([[1, 1], [2, 2], [5, 5]])\n",
        "# predictions = softmax_model.predict(X_test)\n",
        "# print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ihk8ip-T2Hv",
        "outputId": "ad2225ba-32eb-4687-c400-b4ebf18c0ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{('is', 'terrible'), ('movie', 'very'), ('is', 'amazing'), ('very', 'much'), ('love', 'this'), ('I', 'love')}\n",
            "Sentence: I love this movie very much\n",
            "Binary Vector: [0, 1, 0, 1, 1, 1]\n",
            "Sentence: This movie is terrible\n",
            "Binary Vector: [1, 0, 0, 0, 0, 0]\n",
            "Sentence: This movie is amazing\n",
            "Binary Vector: [0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def sentence_to_binary_vector(sentence, unique_bigrams):\n",
        "    words = sentence.split()\n",
        "    sentence_bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "    binary_vector = [1 if bigram in sentence_bigrams else 0 for bigram in unique_bigrams]\n",
        "    return binary_vector\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\"I love this movie very much\", \"This movie is terrible\", \"This movie is amazing\"]\n",
        "labels = [1, 0, 1]  # Example labels\n",
        "\n",
        "# Extract word bi-grams and labels\n",
        "word_bigrams, bigram_labels = extract_word_bigrams_with_labels(sentences, labels)\n",
        "\n",
        "# Extract unique word bi-grams occurring once with labels\n",
        "unique_bigrams_once, unique_labels_once = extract_features(word_bigrams, bigram_labels)\n",
        "print(unique_bigrams_once)\n",
        "# Represent each sentence with a binary vector\n",
        "binary_vectors = [sentence_to_binary_vector(sentence, unique_bigrams_once) for sentence in sentences]\n",
        "\n",
        "# Print the result\n",
        "for sentence, binary_vector in zip(sentences, binary_vectors):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Binary Vector: {binary_vector}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndns-x_Agz_g",
        "outputId": "138c3ef0-635d-47db-93e3-0640dd420f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sum of matrix elements: 154332\n"
          ]
        }
      ],
      "source": [
        "matrix_sum = np.sum(X_train)\n",
        "\n",
        "# Print the sum\n",
        "print(\"Sum of matrix elements:\", matrix_sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOwdHdS4lXYw",
        "outputId": "6961d90d-d8c9-4597-9729-bd41997e6d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 3 3 ... 3 3 3]\n"
          ]
        }
      ],
      "source": [
        "X_test = vectorizer.transform(sst_dataset['test']['tokens'])  # Use transform instead of fit_transform for test data\n",
        "test_predictions = lr_model.predict(X_train)\n",
        "print( test_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aev5haXXlfvM",
        "outputId": "e4ecdf32-2333-4048-d271-641a812560fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8544, 87247)\n",
            "************************\n",
            "(2210, 87247)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(\"************************\")\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmAuenfKjcRV",
        "outputId": "f8986bc2-0e53-40bd-8564-f9894034da67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2723548689138577\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def accuracy(true_labels, predicted_labels):\n",
        "    correct = np.sum(true_labels == predicted_labels)\n",
        "    total = len(true_labels)\n",
        "    return correct / total\n",
        "\n",
        "# Example usage:\n",
        "true_labels = sst_dataset['train']['mapped_label']\n",
        "predicted_labels = test_predictions\n",
        "\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "acc = accuracy(true_labels, predicted_labels)\n",
        "print(\"Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjW5vyxZqP_v",
        "outputId": "c1a9a062-a467-4ae3-b269-7b677b15a884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 4, 3, 2, 3, 4, 4, 3, 4, 3, 4, 2, 4, 2, 4, 3, 3, 2, 4, 3, 4, 3, 2, 4, 2, 2, 4, 3, 4, 1, 4, 3, 2, 3, 4, 3, 3, 3, 2, 3, 3, 4, 2, 3, 2, 0, 3, 3, 1, 4, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 2, 3, 3, 3, 1, 1, 4, 3, 4, 4, 3, 2, 1, 3, 2, 2, 3, 4, 4, 4, 4, 3, 2, 4, 3, 3, 4, 4, 3, 4, 3, 3, 1, 4, 3, 3, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 2, 3, 2, 3, 3, 3, 4, 1, 2, 2, 2, 4, 3, 3, 3, 3, 2, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 2, 4, 3, 2, 1, 3, 2, 3, 3, 1, 4, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 4, 3, 1, 2, 4, 3, 3, 1, 3, 4, 4, 3, 3, 3, 3, 3, 1, 4, 4, 2, 4, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 2, 4, 2, 3, 4, 2, 4, 3, 1, 2, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 1, 3, 4, 3, 3, 4, 4, 3, 3, 4, 3, 3, 2, 4, 2, 3, 4, 2, 2, 4, 3, 3, 2, 3, 4, 3, 3, 4, 3, 4, 3, 3, 2, 4, 1, 4, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, 4, 4, 3, 2, 1, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 2, 1, 3, 2, 4, 4, 3, 4, 3, 3, 3, 2, 3, 3, 2, 4, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 4, 3, 1, 3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 3, 4, 2, 3, 4, 3, 3, 4, 4, 3, 3, 2, 4, 2, 3, 1, 4, 4, 2, 3, 4, 1, 3, 2, 2, 3, 3, 3, 4, 3, 2, 4, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 3, 4, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 2, 4, 1, 3, 0, 4, 3, 4, 4, 4, 4, 3, 2, 3, 4, 3, 3, 3, 4, 4, 3, 2, 4, 2, 4, 0, 4, 3, 3, 0, 3, 4, 4, 2, 3, 3, 4, 2, 2, 1, 4, 4, 4, 3, 4, 2, 4, 3, 3, 3, 2, 3, 3, 3, 3, 3, 4, 3, 3, 3, 2, 4, 3, 1, 4, 2, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 1, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 2, 3, 4, 3, 3, 3, 4, 1, 4, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 2, 3, 2, 3, 3, 4, 3, 3, 2, 4, 3, 4, 2, 4, 3, 3, 3, 4, 3, 2, 4, 4, 1, 3, 1, 2, 2, 2, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 2, 2, 3, 3, 3, 3, 4, 4, 2, 3, 3, 3, 3, 2, 3, 2, 3, 1, 4, 4, 3, 3, 3, 3, 3, 0, 3, 3, 3, 2, 4, 1, 1, 4, 3, 4, 3, 4, 3, 2, 3, 3, 3, 4, 3, 3, 2, 4, 1, 4, 2, 3, 3, 3, 4, 3, 2, 4, 4, 3, 2, 3, 3, 1, 4, 3, 2, 3, 3, 2, 3, 3, 4, 4, 4, 2, 0, 2, 3, 4, 4, 4, 3, 4, 3, 4, 1, 3, 2, 4, 4, 4, 3, 2, 3, 2, 4, 4, 0, 2, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 4, 3, 3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 2, 3, 4, 4, 2, 1, 3, 3, 4, 4, 4, 4, 2, 4, 3, 3, 3, 2, 4, 4, 4, 3, 3, 4, 2, 2, 3, 3, 4, 4, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 4, 4, 4, 1, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 2, 4, 4, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 4, 3, 1, 2, 2, 3, 3, 4, 3, 4, 3, 4, 2, 4, 4, 4, 3, 2, 3, 2, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 2, 2, 2, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 2, 4, 2, 2, 4, 3, 4, 3, 3, 3, 3, 2, 3, 4, 3, 3, 3, 3, 3, 1, 3, 3, 4, 3, 4, 3, 2, 2, 4, 3, 4, 3, 3, 4, 3, 3, 4, 4, 1, 3, 4, 3, 3, 3, 3, 4, 4, 2, 3, 3, 4, 4, 2, 3, 4, 3, 3, 4, 3, 2, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 3, 4, 2, 3, 3, 2, 2, 1, 2, 3, 4, 3, 4, 3, 3, 2, 4, 4, 2, 2, 2, 4, 3, 4, 4, 3, 4, 3, 4, 4, 2, 3, 4, 2, 3, 3, 4, 4, 4, 3, 3, 2, 3, 2, 3, 3, 4, 3, 4, 2, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 0, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 3, 3, 2, 1, 3, 4, 1, 3, 2, 3, 4, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 4, 0, 3, 2, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 4, 1, 2, 3, 3, 4, 4, 4, 2, 3, 4, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 4, 2, 4, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 0, 2, 3, 3, 4, 4, 2, 4, 2, 2, 4, 4, 2, 3, 4, 3, 2, 2, 3, 3, 4, 3, 3, 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 2, 3, 4, 4, 4, 3, 3, 4, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 4, 4, 1, 2, 3, 3, 1, 2, 4, 4, 4, 4, 3, 4, 3, 4, 3, 3, 3, 4, 4, 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 1, 2, 3, 2, 4, 3, 4, 4, 3, 4, 3, 4, 2, 3, 3, 3, 4, 1, 2, 3, 3, 4, 2, 3, 3, 3, 3, 3, 3, 4, 2, 4, 2, 3, 3, 4, 2, 3, 3, 3, 3, 2, 4, 3, 3, 4, 3, 3, 0, 3, 3, 3, 2, 3, 3, 2, 3, 4, 3, 4, 2, 2, 4, 3, 3, 1, 4, 3, 3, 2, 1, 3, 3, 3, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 3, 3, 2, 2, 2, 4, 4, 3, 4, 3, 1, 3, 1, 3, 3, 4, 4, 4, 3, 3, 3, 3, 2, 3, 2, 2, 4, 4, 3, 2, 4, 3, 2, 4, 1, 4, 3, 1, 2, 3, 2, 3, 4, 3, 4, 4, 3, 2, 3, 4, 2, 3, 1, 4, 4, 4, 4, 2, 4, 3, 3, 3, 4, 2, 3, 3, 3, 2, 3, 3, 4, 4, 4, 2, 3, 3, 4, 4, 4, 1, 2, 3, 3, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 3, 3, 2, 3, 3, 1, 2, 1, 3, 2, 3, 2, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 4, 3, 3, 1, 3, 3, 3, 4, 3, 3, 2, 3, 2, 3, 3, 2, 1, 3, 3, 3, 4, 3, 3, 2, 4, 3, 3, 2, 2, 1, 0, 4, 3, 4, 3, 2, 3, 4, 3, 3, 3, 3, 4, 2, 4, 3, 3, 2, 3, 3, 2, 3, 3, 4, 4, 4, 2, 4, 4, 4, 3, 1, 2, 3, 2, 4, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 3, 2, 3, 4, 1, 2, 2, 4, 3, 3, 4, 3, 3, 4, 2, 3, 1, 1, 2, 3, 2, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 1, 2, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, 2, 3, 3, 3, 3, 2, 1, 4, 3, 2, 3, 3, 2, 3, 4, 3, 4, 4, 3, 4, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 2, 2, 3, 4, 3, 4, 3, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 4, 3, 4, 4, 2, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 2, 3, 4, 3, 2, 3, 2, 3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3, 1, 3, 3, 3, 4, 3, 3, 3, 4, 4, 3, 2, 3, 3, 4, 3, 2, 3, 3, 4, 4, 3, 3, 4, 4, 2, 3, 4, 3, 2, 2, 3, 2, 4, 3, 2, 4, 3, 1, 4, 3, 4, 3, 2, 3, 3, 4, 4, 2, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 1, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 2, 2, 3, 4, 2, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 2, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 1, 4, 3, 4, 2, 2, 2, 4, 3, 3, 3, 2, 4, 4, 3, 3, 4, 3, 4, 4, 2, 3, 3, 4, 3, 4, 4, 3, 2, 4, 4, 4, 3, 2, 1, 2, 3, 3, 4, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 4, 2, 2, 3, 3, 2, 3, 3, 2, 4, 3, 2, 3, 4, 2, 2, 3, 3, 3, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 1, 4, 3, 4, 4, 4, 3, 4, 3, 3, 4, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 2, 3, 3, 2, 4, 2, 3, 3, 2, 1, 3, 3, 3, 3, 3, 2, 4, 4, 3, 4, 1, 4, 4, 2, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 1, 3, 3, 3, 2, 2, 0, 3, 3, 2, 4, 3, 3, 4, 3, 3, 2, 4, 4, 4, 2, 3, 2, 3, 3, 3, 3, 3, 3, 4, 4, 2, 3, 2, 4, 4, 2, 1, 3, 1, 2, 4, 2, 3, 4, 3, 4, 4, 3, 4, 3, 1, 3, 3, 4, 4, 2, 2, 3, 4, 4, 3, 1, 4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 4, 3, 4, 3, 2, 2, 4, 4, 4, 1, 1, 4, 1, 3, 3, 4, 2, 4, 4, 3, 4, 1, 2, 2, 4, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 2, 3, 3, 2, 2, 3, 3, 4, 4, 4, 3, 4, 0, 3, 4, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 2, 4, 3, 2, 4, 4, 4, 3, 2, 3, 3, 3, 2, 2, 2, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 2, 3, 3, 4, 4, 4, 1, 2, 2, 3, 3, 4, 3, 3, 3, 3, 4, 3, 1, 3, 3, 2, 3, 4, 4, 1, 3, 2, 4, 3, 4, 3, 3, 4, 3, 2, 4, 3, 2, 3, 4, 3, 1, 4, 3, 3, 1, 3, 4, 2, 3, 3, 4, 3, 3, 2, 3, 2, 3, 4, 2, 3, 3, 1, 3, 2, 4, 3, 4, 3, 3, 3, 1, 2, 1, 2, 4, 3, 3, 2, 4, 3, 0, 3, 4, 1, 2, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 2, 4, 4, 4, 1, 2, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 1, 2, 2, 2, 1, 3, 3, 4, 2, 2, 3, 4, 4, 4, 4, 2, 3, 3, 3, 2, 4, 1, 1, 4, 2, 2, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 3, 2, 4, 4, 3, 3, 4, 4, 3, 3, 2, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 1, 4, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 4, 3, 4, 3, 3, 2, 3, 3, 4, 4, 3, 2, 3, 4, 4, 4, 3, 3, 3, 2, 1, 2, 3, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 4, 0, 3, 1, 3, 4, 2, 3, 4, 2, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 1, 4, 4, 3, 3, 4, 2, 3, 4, 4, 3, 3, 4, 3, 2, 3, 4, 2, 3, 1, 1, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 2, 2, 4, 3, 3, 3, 4, 3, 1, 2, 3, 4, 3, 4, 4, 4, 2, 3, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 1, 3, 3, 2, 3, 4, 4, 3, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 1, 3, 3, 4, 2, 3, 4, 3, 2, 3, 3, 2, 3, 2, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 4, 3, 3, 4, 2, 2, 4, 3, 2, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 4, 2, 2, 4, 3, 3, 4, 2, 3, 3, 3, 4, 3, 2, 1, 3, 3, 4, 4, 4, 4, 3, 3, 3, 2, 4, 4, 4, 3, 2, 3, 1, 4, 3, 4, 4, 3, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 2, 3, 3, 2, 2, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 2, 4, 3, 3, 0, 4, 4, 1, 4, 4, 3, 4, 3, 4, 3, 3, 2, 3, 3, 4, 4, 3, 3, 3, 4, 4, 2, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 4, 2, 3, 3, 2, 3, 4, 3, 3, 3, 4, 1, 4, 3, 4, 3, 2, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 2, 4, 3, 2, 4, 4, 3, 3, 4, 3, 4, 3, 3, 0, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 2, 3, 4, 4, 3, 3, 4, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 4, 4, 3, 4, 3, 4, 3, 4, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 2, 4, 2, 3, 3, 4, 3, 4, 4, 3, 3, 2, 2, 3, 4, 4, 3, 3, 4, 3, 2, 4, 3, 4, 3, 2, 4, 2, 3, 4, 3, 2, 4, 2, 1, 2, 3, 4, 2, 3, 3, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 2, 4, 3, 4, 1, 3, 2, 3, 4, 4, 0, 2, 3, 4, 4, 3, 4, 2, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 2, 4, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 4, 4, 0, 3, 4, 2, 2, 3, 3, 4, 3, 4, 2, 4, 4, 4, 3, 3, 2, 3, 4, 3, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 2, 3, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 3, 3, 2, 2, 3, 4, 3, 4, 3, 4, 2, 4, 3, 3, 3, 4, 3, 4, 3, 3, 4, 4, 2, 3, 2, 3, 3, 3, 3, 4, 4, 2, 3, 4, 3, 3, 1, 4, 3, 4, 3, 2, 4, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 2, 0, 2, 3, 4, 3, 4, 4, 4, 3, 3, 2, 3, 2, 4, 3, 4, 3, 4, 3, 3, 2, 3, 4, 3, 2, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 1, 3, 3, 3, 4, 3, 2, 2, 4, 3, 1, 4, 4, 3, 3, 3, 4, 4, 3, 4, 3, 4, 2, 4, 3, 1, 2, 3, 3, 2, 3, 2, 3, 4, 1, 3, 3, 4, 3, 3, 4, 4, 4, 2, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 3, 1, 4, 4, 1, 3, 3, 4, 3, 2, 2, 3, 2, 2, 3, 1, 2, 3, 4, 4, 4, 3, 3, 3, 2, 3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 4, 4, 3, 4, 4, 2, 3, 3, 3, 2, 3, 2, 4, 2, 4, 2, 4, 3, 4, 2, 1, 2, 4, 1, 4, 4, 3, 2, 2, 3, 3, 4, 3, 2, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 2, 4, 2, 2, 2, 3, 3, 4, 3, 3, 3, 2, 2, 4, 3, 3, 2, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 2, 4, 3, 4, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 4, 3, 3, 3, 3, 1, 2, 2, 3, 3, 4, 4, 4, 2, 3, 2, 2, 3, 3, 3, 3, 4, 3, 2, 3, 3, 4, 3, 4, 2, 4, 1, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 4, 2, 3, 3, 4, 3, 4, 3, 3, 2, 1, 2, 4, 4, 4, 4, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 4, 4, 3, 1, 4, 2, 3, 4, 2, 3, 2, 4, 1, 3, 3, 1, 3, 4, 2, 3, 3, 4, 3, 2, 3, 3, 4, 2, 2, 2, 3, 1, 0, 4, 1, 3, 3, 4, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 2, 2, 2, 3, 4, 4, 2, 2, 4, 4, 4, 2, 2, 3, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 2, 4, 2, 2, 4, 4, 3, 1, 3, 2, 4, 3, 3, 3, 4, 3, 3, 4, 2, 4, 1, 3, 4, 3, 3, 2, 3, 3, 2, 3, 3, 4, 2, 3, 2, 3, 4, 3, 3, 4, 2, 3, 3, 2, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 4, 4, 3, 3, 4, 4, 1, 3, 3, 4, 3, 4, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 4, 3, 0, 3, 3, 3, 3, 3, 2, 4, 3, 2, 4, 4, 2, 3, 4, 3, 3, 4, 2, 3, 3, 3, 1, 3, 3, 3, 4, 3, 4, 4, 3, 4, 1, 4, 3, 2, 3, 3, 3, 4, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 4, 2, 2, 4, 4, 3, 3, 4, 3, 3, 3, 2, 4, 4, 4, 2, 4, 3, 4, 3, 3, 3, 2, 2, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 2, 3, 4, 4, 3, 4, 4, 1, 4, 3, 1, 4, 4, 4, 3, 4, 3, 4, 4, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 4, 2, 3, 3, 3, 1, 1, 3, 3, 1, 4, 2, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 2, 2, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 2, 4, 4, 4, 3, 2, 3, 2, 2, 1, 2, 1, 1, 3, 1, 3, 4, 3, 3, 3, 2, 2, 3, 2, 3, 3, 1, 3, 4, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 1, 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 4, 3, 4, 1, 3, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 2, 3, 3, 4, 3, 1, 3, 3, 3, 2, 4, 3, 4, 4, 3, 2, 1, 2, 4, 2, 3, 0, 1, 4, 3, 1, 3, 3, 3, 3, 2, 2, 2, 4, 2, 3, 3, 1, 2, 1, 3, 3, 2, 4, 4, 3, 1, 4, 3, 1, 3, 4, 3, 2, 3, 3, 4, 4, 2, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 4, 3, 3, 3, 2, 1, 4, 3, 2, 3, 4, 3, 1, 3, 2, 3, 4, 4, 3, 2, 4, 4, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 4, 2, 4, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 4, 3, 1, 0, 4, 1, 4, 2, 4, 3, 3, 3, 2, 3, 3, 3, 3, 2, 1, 3, 4, 2, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 1, 3, 2, 3, 3, 4, 1, 4, 3, 2, 2, 1, 3, 4, 3, 4, 2, 1, 4, 3, 4, 3, 0, 3, 3, 2, 4, 4, 3, 4, 2, 3, 4, 4, 3, 4, 3, 1, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 2, 2, 2, 3, 2, 3, 4, 3, 4, 3, 4, 1, 3, 2, 3, 2, 3, 4, 2, 4, 3, 3, 3, 3, 3, 4, 2, 3, 3, 4, 3, 2, 4, 4, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 4, 0, 3, 2, 3, 3, 4, 4, 3, 4, 3, 4, 1, 2, 3, 4, 4, 3, 3, 2, 4, 3, 4, 2, 4, 4, 3, 2, 4, 1, 3, 2, 3, 4, 2, 1, 2, 4, 4, 1, 3, 4, 3, 4, 3, 4, 2, 4, 3, 1, 3, 4, 4, 0, 3, 3, 4, 4, 4, 3, 4, 1, 3, 4, 4, 3, 4, 3, 4, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 1, 2, 3, 1, 4, 1, 3, 4, 3, 3, 2, 4, 4, 3, 2, 3, 2, 3, 3, 4, 1, 3, 4, 2, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 2, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 4, 0, 1, 1, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 0, 2, 1, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 2, 0, 2, 1, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 2, 1, 0, 1, 3, 1, 1, 1, 0, 1, 1, 3, 2, 1, 1, 3, 3, 1, 0, 1, 1, 2, 1, 2, 1, 2, 0, 1, 1, 2, 2, 0, 0, 1, 3, 3, 1, 0, 3, 0, 1, 0, 2, 0, 0, 2, 1, 0, 1, 1, 2, 0, 1, 0, 1, 1, 0, 2, 2, 1, 2, 2, 0, 3, 2, 1, 1, 0, 1, 3, 2, 0, 1, 3, 3, 2, 0, 0, 1, 1, 2, 0, 0, 1, 3, 2, 2, 1, 1, 2, 2, 3, 2, 1, 1, 1, 2, 0, 1, 1, 1, 3, 1, 2, 3, 1, 2, 1, 2, 1, 1, 0, 0, 1, 1, 2, 1, 2, 2, 0, 1, 3, 0, 2, 0, 0, 0, 2, 0, 1, 0, 2, 1, 1, 3, 1, 0, 2, 1, 1, 2, 1, 1, 1, 3, 0, 2, 1, 1, 2, 2, 1, 1, 2, 0, 2, 1, 1, 2, 3, 1, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 2, 2, 0, 0, 1, 3, 1, 1, 2, 3, 1, 0, 0, 2, 2, 1, 1, 0, 2, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 3, 4, 0, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 3, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 3, 1, 1, 1, 2, 2, 1, 2, 0, 3, 1, 0, 0, 1, 3, 0, 2, 0, 0, 3, 2, 2, 2, 2, 1, 1, 0, 1, 1, 3, 2, 2, 0, 1, 1, 1, 0, 0, 2, 3, 1, 1, 2, 0, 1, 3, 2, 1, 4, 1, 0, 1, 0, 1, 1, 3, 0, 2, 1, 0, 1, 3, 0, 3, 1, 1, 1, 2, 0, 1, 2, 2, 0, 3, 0, 2, 4, 0, 3, 2, 0, 2, 1, 2, 2, 1, 1, 1, 0, 1, 3, 1, 0, 2, 1, 3, 3, 3, 1, 0, 1, 1, 1, 2, 0, 3, 0, 3, 2, 1, 2, 2, 3, 2, 0, 2, 1, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 1, 2, 1, 0, 3, 1, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 2, 0, 1, 3, 2, 1, 3, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 2, 4, 1, 1, 4, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 0, 2, 3, 1, 0, 0, 2, 3, 3, 1, 1, 2, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 2, 3, 3, 2, 1, 0, 1, 1, 0, 2, 2, 1, 1, 1, 1, 2, 0, 2, 2, 4, 0, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 2, 0, 2, 0, 0, 1, 1, 0, 1, 2, 1, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 2, 1, 0, 1, 3, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 0, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 0, 1, 1, 3, 0, 3, 1, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 3, 2, 2, 2, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 2, 1, 3, 2, 0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 4, 1, 1, 2, 2, 0, 1, 2, 1, 2, 0, 0, 1, 3, 2, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 1, 0, 3, 1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 0, 3, 1, 1, 2, 0, 2, 1, 1, 2, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2, 1, 0, 2, 0, 2, 0, 1, 2, 2, 0, 3, 1, 1, 1, 2, 1, 1, 2, 2, 0, 0, 2, 1, 3, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 0, 1, 1, 2, 0, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 0, 1, 0, 4, 0, 3, 1, 1, 1, 3, 1, 1, 0, 2, 0, 3, 2, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 0, 1, 1, 3, 0, 2, 2, 0, 1, 0, 2, 0, 1, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 3, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 3, 1, 0, 3, 2, 1, 1, 3, 1, 3, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 3, 0, 0, 1, 0, 1, 2, 0, 3, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 0, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 2, 1, 2, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 2, 0, 0, 0, 1, 1, 0, 1, 3, 1, 2, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 4, 1, 2, 0, 2, 0, 2, 3, 0, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 1, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 3, 1, 1, 1, 0, 3, 1, 0, 0, 0, 0, 1, 0, 0, 3, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 0, 1, 2, 0, 3, 1, 0, 1, 1, 1, 1, 1, 4, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 3, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 1, 0, 3, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 2, 0, 2, 3, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 0, 0, 0, 4, 2, 2, 1, 1, 1, 2, 2, 0, 1, 3, 0, 0, 1, 1, 3, 1, 2, 2, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 1, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 2, 0, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 2, 1, 2, 3, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 3, 1, 1, 1, 3, 4, 1, 1, 2, 0, 0, 1, 1, 2, 2, 3, 1, 3, 2, 0, 2, 1, 1, 0, 2, 2, 2, 0, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 3, 1, 1, 0, 1, 0, 1, 3, 2, 1, 1, 1, 3, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 3, 1, 1, 0, 0, 1, 0, 1, 0, 3, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 3, 1, 2, 1, 0, 1, 1, 0, 1, 2, 3, 0, 1, 3, 0, 2, 2, 1, 3, 1, 1, 0, 3, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 0, 0, 2, 3, 2, 0, 2, 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 3, 1, 2, 4, 1, 2, 2, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 3, 2, 2, 1, 3, 4, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 4, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 4, 2, 1, 3, 2, 0, 2, 2, 0, 0, 1, 3, 1, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 2, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 2, 1, 1, 0, 1, 2, 0, 2, 2, 1, 0, 1, 0, 2, 1, 0, 3, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 0, 1, 2, 1, 3, 3, 0, 0, 1, 0, 2, 2, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 0, 3, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 0, 3, 1, 0, 1, 1, 1, 3, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 2, 1, 3, 1, 0, 1, 2, 2, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 1, 3, 1, 1, 2, 1, 0, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1, 3, 1, 3, 2, 2, 0, 0, 3, 4, 0, 3, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 2, 2, 2, 2, 2, 0, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 2, 2, 1, 1, 1, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 2, 1, 0, 2, 1, 2, 1, 3, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 4, 3, 0, 1, 3, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 3, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 3, 2, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 2, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 0, 3, 1, 1, 3, 1, 2, 4, 2, 0, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 3, 0, 0, 1, 0, 1, 0, 1, 4, 0, 2, 0, 1, 1, 1, 2, 1, 3, 2, 2, 1, 1, 1, 2, 0, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 3, 2, 4, 0, 3, 2, 1, 3, 1, 0, 2, 0, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 3, 1, 2, 0, 1, 2, 1, 1, 2, 3, 1, 0, 0, 1, 1, 3, 0, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0, 1, 2, 3, 1, 1, 2, 2, 0, 2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 2, 1, 0, 3, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 2, 3, 1, 1, 2, 1, 3, 1, 1, 2, 2, 3, 1, 1, 0, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 3, 1, 1, 2, 1, 1, 0, 2, 2, 0, 3, 4, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 3, 1, 3, 1, 0, 1, 2, 2, 1, 2, 1, 3, 2, 2, 3, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 3, 1, 0, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 0, 2, 1, 1, 1, 3, 1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 0, 2, 0, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 0, 3, 3, 0, 2, 0, 1, 2, 1, 1, 1, 1, 3, 1, 0, 1, 0, 1, 1, 2, 1, 2, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1, 1, 3, 0, 2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 3, 0, 0, 1, 2, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 2, 1, 1, 0, 0, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 3, 2, 1, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 3, 1, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 0, 1, 1, 3, 2, 2, 0, 1, 2, 2, 2, 2, 3, 2, 1, 0, 1, 3, 1, 4, 3, 1, 0, 2, 0, 0, 0, 3, 2, 0, 2, 0, 0, 2, 1, 0, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 2, 2, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 2, 2, 0, 1, 2, 1, 2, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 2, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 2, 2, 0, 1, 2, 1, 2, 0, 0, 1, 1, 0, 1, 1, 1, 2, 2, 3, 0, 2, 0, 4, 0, 0, 1, 1, 2, 2, 3, 0, 3, 1, 1, 0, 2, 1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 3, 0, 2, 0, 0, 1, 1, 0, 3, 2, 0, 1, 3, 1, 1, 0, 2, 3, 1, 0, 2, 0, 1, 2, 1, 0, 1, 0, 4, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 0, 2, 1, 1, 3, 1, 0, 0, 1, 3, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 0, 3, 2, 1, 1, 2, 1, 2, 1, 2, 0, 1, 1, 1, 3, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2, 0, 0, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 2, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 1, 3, 1, 1, 1, 0, 3, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 3, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2, 2, 2, 0, 1, 1, 0, 0, 3, 1, 1, 0, 2, 1, 1, 0, 1, 0, 2, 0, 3, 1, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 0, 3, 1, 2, 1, 3, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 2, 3, 0, 1, 1, 2, 2, 1, 1, 1, 2, 2, 0, 1, 0, 1, 3, 1, 3, 1, 1, 3, 4, 2, 2, 0, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 3, 1, 2, 0, 2, 3, 2, 1, 1, 2, 3, 0, 2, 0, 0, 2, 1, 0, 1, 2, 3, 1, 1, 2, 2, 2, 0, 0, 1, 0, 0, 2, 2, 2, 2, 0, 1, 1, 0, 3, 1, 1, 2, 2, 4, 2, 2, 1, 3, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 3, 3, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 3, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 2, 1, 2, 2, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 3, 1, 1, 2, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 2, 1, 3, 2, 1, 1, 1, 2, 1, 3, 2, 2, 1, 2, 1, 2, 2, 2, 3, 0, 1, 3, 0, 0, 1, 3, 0, 1, 1, 1, 2, 0, 0, 0, 1, 2, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 3, 1, 1, 2, 1, 1, 0, 3, 0, 0, 1, 1, 2, 1, 1, 3, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 2, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 2, 0, 3, 1, 1, 1, 0, 1, 2, 3, 0, 1, 2, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 2, 0, 1, 0, 1, 3, 1, 1, 1, 0, 0, 0, 3, 0, 1, 3, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 2, 1, 1, 3, 1, 1, 0, 1, 2, 2, 1, 2, 2, 1, 0, 1, 1, 3, 1, 1, 0, 0, 3, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 4, 2, 2, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 3, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 3, 1, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 0, 1, 1, 3, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1, 2, 1, 3, 0, 2, 1, 1, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 2, 0, 1, 3, 1, 1, 1, 0, 1, 2, 2, 3, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 2, 1, 3, 4, 3, 4, 2, 1, 3, 2, 4, 2, 3, 3, 3, 3, 3, 4, 3, 2, 1, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 2, 3, 4, 3, 0, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 3, 0, 3, 3, 2, 1, 2, 2, 1, 0, 2, 2, 2, 2, 1, 2, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 1, 1, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 4, 4, 3, 4, 3, 4, 4, 3, 4, 1, 4, 4, 4, 4, 3, 3, 4, 2, 3, 4, 4, 4, 2, 2, 4, 3, 4, 3, 3, 4, 2, 3, 4, 4, 3, 4, 3, 4, 4, 3, 2, 3, 3, 4, 4, 4, 3, 1, 4, 3, 3, 3, 3, 2, 4, 3, 4, 3, 1, 3, 3, 4, 4, 4, 1, 4, 4, 4, 4, 0, 3, 3, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 1, 2, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 3, 0, 2, 0, 1, 0, 1, 1, 0, 0, 2, 2, 1, 2, 2, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 2, 0, 3, 0, 1, 3, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "print(true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtdUe4I8pEva",
        "outputId": "397fa543-78a9-488c-bfdd-4394e46c1e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[   0.    0.    0. 1092.    0.]\n",
            " [   0.    5.    0. 2213.    0.]\n",
            " [   0.    0.    0. 1624.    0.]\n",
            " [   0.    0.    0. 2322.    0.]\n",
            " [   0.    0.    0. 1288.    0.]]\n",
            "Precision per class: [0.        1.        0.        0.2719288 0.       ]\n",
            "Recall per class: [0.         0.00225428 0.         1.         0.        ]\n",
            "F1 score per class: [0.         0.00449843 0.         0.42758494 0.        ]\n",
            "Macro-averaged Precision: 0.25438575945661085\n",
            "Macro-averaged Recall: 0.20045085662759243\n",
            "Macro-averaged F1 score: 0.08641667249627165\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix(true_labels, predicted_labels, num_classes):\n",
        "    conf_mat = np.zeros((num_classes, num_classes))\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        conf_mat[true, pred] += 1\n",
        "    return conf_mat\n",
        "\n",
        "def precision_recall_f1(conf_mat):\n",
        "    num_classes = conf_mat.shape[0]\n",
        "    precision = np.zeros(num_classes)\n",
        "    recall = np.zeros(num_classes)\n",
        "    f1 = np.zeros(num_classes)\n",
        "    for i in range(num_classes):\n",
        "        tp = conf_mat[i, i]\n",
        "        fp = np.sum(conf_mat[:, i]) - tp\n",
        "        fn = np.sum(conf_mat[i, :]) - tp\n",
        "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def macro_average(metric):\n",
        "    return np.mean(metric)\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "conf_mat = confusion_matrix(true_labels, predicted_labels, num_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "\n",
        "precision, recall, f1 = precision_recall_f1(conf_mat)\n",
        "print(\"Precision per class:\", precision)\n",
        "print(\"Recall per class:\", recall)\n",
        "print(\"F1 score per class:\", f1)\n",
        "\n",
        "macro_precision = macro_average(precision)\n",
        "macro_recall = macro_average(recall)\n",
        "macro_f1 = macro_average(f1)\n",
        "print(\"Macro-averaged Precision:\", macro_precision)\n",
        "print(\"Macro-averaged Recall:\", macro_recall)\n",
        "print(\"Macro-averaged F1 score:\", macro_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWl71R61ps7w",
        "outputId": "a1cc2577-9b14-4c6a-83e0-01a81f1971e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0  96   0 183   0]\n",
            " [  0 238   0 395   0]\n",
            " [  0  94   0 295   0]\n",
            " [  0  84   0 426   0]\n",
            " [  0  47   0 352   0]]\n",
            "Class 0: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
            "Class 1: Precision=0.4258, Recall=0.3760, F1-Score=0.3993\n",
            "Class 2: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
            "Class 3: Precision=0.2580, Recall=0.8353, F1-Score=0.3943\n",
            "Class 4: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
            "Macro-averaged Precision: 0.1368\n",
            "Macro-averaged Recall: 0.2423\n",
            "Macro-averaged F1-Score: 0.1587\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, num_classes):\n",
        "    \"\"\"\n",
        "    Generate the confusion matrix given the predictions and the ground truth labels.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Array of true labels.\n",
        "    - y_pred: Array of predicted labels.\n",
        "    - num_classes: Number of classes in the classification task.\n",
        "\n",
        "    Returns:\n",
        "    - cm: Confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
        "    for true_label, pred_label in zip(y_true, y_pred):\n",
        "        cm[true_label, pred_label] += 1\n",
        "    return cm\n",
        "\n",
        "def compute_metrics(confusion_matrix):\n",
        "    \"\"\"\n",
        "    Compute precision, recall, and F1 score per class and macro-averaged from the confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - confusion_matrix: Confusion matrix.\n",
        "\n",
        "    Returns:\n",
        "    - precision: Array of precision values per class.\n",
        "    - recall: Array of recall values per class.\n",
        "    - f1_score: Array of F1 score values per class.\n",
        "    - macro_precision: Macro-averaged precision.\n",
        "    - macro_recall: Macro-averaged recall.\n",
        "    - macro_f1_score: Macro-averaged F1 score.\n",
        "    \"\"\"\n",
        "    num_classes = confusion_matrix.shape[0]\n",
        "    precision = np.zeros(num_classes)\n",
        "    recall = np.zeros(num_classes)\n",
        "    f1_score = np.zeros(num_classes)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        tp = confusion_matrix[i, i]\n",
        "        fp = np.sum(confusion_matrix[:, i]) - tp\n",
        "        fn = np.sum(confusion_matrix[i, :]) - tp\n",
        "\n",
        "        precision[i] = tp / (tp + fp) if tp + fp != 0 else 0\n",
        "        recall[i] = tp / (tp + fn) if tp + fn != 0 else 0\n",
        "        f1_score[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i]) if precision[i] + recall[i] != 0 else 0\n",
        "\n",
        "    macro_precision = np.mean(precision)\n",
        "    macro_recall = np.mean(recall)\n",
        "    macro_f1_score = np.mean(f1_score)\n",
        "\n",
        "    return precision, recall, f1_score, macro_precision, macro_recall, macro_f1_score\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming y_true and y_pred are the ground truth and predicted labels respectively\n",
        "\n",
        "# Step 1: Generate confusion matrix\n",
        "num_classes = 5  # Assuming 5 classes\n",
        "cm = confusion_matrix(true_labels, predicted_labels, num_classes)\n",
        "print(cm)\n",
        "# Step 2: Compute metrics\n",
        "precision, recall, f1_score, macro_precision, macro_recall, macro_f1_score = compute_metrics(cm)\n",
        "\n",
        "# Print metrics\n",
        "for i in range(num_classes):\n",
        "    print(f\"Class {i}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1-Score={f1_score[i]:.4f}\")\n",
        "\n",
        "print(f\"Macro-averaged Precision: {macro_precision:.4f}\")\n",
        "print(f\"Macro-averaged Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro-averaged F1-Score: {macro_f1_score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}