{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elmahygurl/NLP_sst_Classification/blob/main/NLP_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting dataset"
      ],
      "metadata": {
        "id": "zhv37jNcClyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne9toyr5CkLp",
        "outputId": "1996ec8b-9ed5-451e-f9a1-3ede7640f7c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n"
      ],
      "metadata": {
        "id": "z3D5D_RoDAhW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sst_dataset = load_dataset('sst')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NdKuRUsDJw7",
        "outputId": "ad4ba597-7589-4a3d-f134-d284407a6935"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sst_dataset['train'][500])\n",
        "print(f\"Number of training examples: {len(sst_dataset['train'])}\")\n",
        "print(f\"Number of validation examples: {len(sst_dataset['validation'])}\")\n",
        "print(f\"Number of test examples: {len(sst_dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xAHI1AxDKqm",
        "outputId": "0fd39a67-edc1-4242-abf2-4d71e902f1bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'This story still seems timely and important .', 'label': 0.6944400072097778, 'tokens': 'This|story|still|seems|timely|and|important|.', 'tree': '14|14|13|11|9|9|10|12|10|11|12|13|15|15|0'}\n",
            "Number of training examples: 8544\n",
            "Number of validation examples: 1101\n",
            "Number of test examples: 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "9IHBlanSB3eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the 'tree' column and adjusting the labels to accomodate the 5 classes we want to classify to where:\n",
        "From 0 to 0.2 (0.2 included) will be class 0 “very negative”.\n",
        "\n",
        "From 0.2 to 0.4 (0.4 included) will be class 1 “negative”.\n",
        "\n",
        "From 0.4 to 0.6 (0.6 included) will be class 2 “neutral”.\n",
        "\n",
        "From 0.6 to 0.8 (0.8 included) will be class 3 “positive”.\n",
        "\n",
        "From 0.8 to 1.0 (1.0 included) will be class 4 “very positive”."
      ],
      "metadata": {
        "id": "a6rfAZ8_TfEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting splits to Pandas DataFrame for manipulation\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train'])\n",
        "test_df = pd.DataFrame(sst_dataset['test'])\n",
        "val_df = pd.DataFrame(sst_dataset['validation'])\n",
        "\n",
        "#function to map float labels to our desired categorical classes\n",
        "def map_labels_to_categories(label):\n",
        "    if 0.0 <= label <= 0.2:\n",
        "        return 0  # very negative\n",
        "    elif 0.2 < label <= 0.4:\n",
        "        return 1  # negative\n",
        "    elif 0.4 < label <= 0.6:\n",
        "        return 2  # neutral\n",
        "    elif 0.6 < label <= 0.8:\n",
        "        return 3  # positive\n",
        "    elif 0.8 < label <= 1.0:\n",
        "        return 4  # very positive\n",
        "\n",
        "#mapping function to create a new column with mapped labels\n",
        "train_df['mapped_label'] = train_df['label'].apply(map_labels_to_categories)\n",
        "test_df['mapped_label'] = test_df['label'].apply(map_labels_to_categories)\n",
        "val_df['mapped_label'] = val_df['label'].apply(map_labels_to_categories)\n",
        "\n",
        "#dropping the original label column\n",
        "train_df = train_df.drop('label', axis=1)\n",
        "test_df = test_df.drop('label', axis=1)\n",
        "val_df = val_df.drop('label', axis=1)\n",
        "\n",
        "train_df = train_df.drop(columns=['tree']) #dropping 'tree' column\n",
        "test_df = test_df.drop(columns=['tree'])\n",
        "val_df = val_df.drop(columns=['tree'])\n",
        "\n",
        "######just to print an output and visualise\n",
        "#convert the DataFrame back to the datasets format\n",
        "sst_dataset['train'] = Dataset.from_pandas(train_df)\n",
        "sst_dataset['test'] = Dataset.from_pandas(test_df)\n",
        "sst_dataset['validation'] = Dataset.from_pandas(val_df)\n",
        "\n",
        "#display the updated dataset\n",
        "print(sst_dataset)"
      ],
      "metadata": {
        "id": "s5EPCqgimpFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86344073-e298-4a7c-81e5-3e829080dba1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 8544\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 1101\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'tokens', 'mapped_label'],\n",
            "        num_rows: 2210\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyzing the distribution of labels\n",
        "train_label_distribution = train_df['mapped_label'].value_counts().sort_index()\n",
        "test_label_distribution = test_df['mapped_label'].value_counts().sort_index()\n",
        "val_label_distribution = val_df['mapped_label'].value_counts().sort_index()\n",
        "\n",
        "print(\"Training Split Label Distribution:\")\n",
        "print(train_label_distribution)\n",
        "\n",
        "print(\"\\nTest Split Label Distribution:\")\n",
        "print(test_label_distribution)\n",
        "\n",
        "print(\"\\nValidation Split Label Distribution:\")\n",
        "print(val_label_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688DssCrsigk",
        "outputId": "9eec0b81-9c2f-4bcd-b01f-8297090cca4a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Split Label Distribution:\n",
            "0    1092\n",
            "1    2218\n",
            "2    1624\n",
            "3    2322\n",
            "4    1288\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Test Split Label Distribution:\n",
            "0    279\n",
            "1    633\n",
            "2    389\n",
            "3    510\n",
            "4    399\n",
            "Name: mapped_label, dtype: int64\n",
            "\n",
            "Validation Split Label Distribution:\n",
            "0    139\n",
            "1    289\n",
            "2    229\n",
            "3    279\n",
            "4    165\n",
            "Name: mapped_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisation for us"
      ],
      "metadata": {
        "id": "GbDu1oUutxsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(sst_dataset['train']) #convert the 'train' split to a Pandas DataFrame\n",
        "\n",
        "# Plot the distribution of mapped labels in the 'train' split\n",
        "train_df['mapped_label'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Mapped Labels in the Train Split')\n",
        "plt.xlabel('Mapped Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "5ZmT1vVBd1Fy",
        "outputId": "6c431a5c-3692-4078-c2f4-5b72f1f24a8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print first few rows of the 'train' split to see the transformed data\n",
        "print(train_df.head())\n",
        "\n",
        "#to ensure correct mapping\n",
        "print(\"Unique values in mapped_labels = \",train_df['mapped_label'].unique())\n",
        "\n",
        "#check for missing values in the 'mapped_label' column of the 'train' split\n",
        "print(\"Missing = \",train_df['mapped_label'].isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_iFiVO7TAe6",
        "outputId": "61dad42b-109e-4db0-992e-b113404b4a89"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  \\\n",
            "0  The Rock is destined to be the 21st Century 's...   \n",
            "1  The gorgeously elaborate continuation of `` Th...   \n",
            "2  Singer\\/composer Bryan Adams contributes a sle...   \n",
            "3  You 'd think by now America would have had eno...   \n",
            "4               Yet the act is still charming here .   \n",
            "\n",
            "                                              tokens  mapped_label  \n",
            "0  The|Rock|is|destined|to|be|the|21st|Century|'s...             3  \n",
            "1  The|gorgeously|elaborate|continuation|of|``|Th...             4  \n",
            "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...             3  \n",
            "3  You|'d|think|by|now|America|would|have|had|eno...             2  \n",
            "4               Yet|the|act|is|still|charming|here|.             3  \n",
            "Unique values in mapped_labels =  [3 4 2 1 0]\n",
            "Missing =  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n"
      ],
      "metadata": {
        "id": "Ngi3yEz6q7rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def train_naive_bayes(D, C):\n",
        "    Ndoc = len(D)   #total number of documents in training dataset D\n",
        "    Nc = np.array([np.sum(D['mapped_label'] == c) for c in C]) #calculates no. of documents in D that belong to each class in C\n",
        "    logprior = np.log(Nc / Ndoc) #calculating prior probability of each class\n",
        "\n",
        "    #building vocabulary of D\n",
        "    V = set()\n",
        "    for doc in D['sentence']:\n",
        "        V.update(doc.split())  #adding to V so it has unique words\n",
        "    V = list(V) #converting to list\n",
        "\n",
        "    #calculating P(w|c) terms\n",
        "    bigdoc = {}  #new dictionary\n",
        "    for c in C:\n",
        "        bigdoc[c] = ' '.join(D[D['mapped_label'] == c]['sentence']).split() #concatenate sentences of documents belonging to each class and split them into words\n",
        "\n",
        "    #calculating frequency of each word in vocabulary for each class and store counts in a 2D NumPy array count\n",
        "    count = np.zeros((len(V), len(C)))\n",
        "    for i, word in enumerate(V):\n",
        "        for j, c in enumerate(C):\n",
        "            count[i, j] = bigdoc[c].count(word)\n",
        "\n",
        "    #calculate log likelihoods of each word given each class\n",
        "    #using Laplace smoothing to avoid zero probabilities\n",
        "    loglikelihood = np.log((count + 1) / (np.sum(count, axis=0, keepdims=True) + len(V)))\n",
        "\n",
        "    return logprior, loglikelihood, V\n",
        "\n",
        "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
        "    sum_ = np.zeros(len(C))  # to store log probabilities of each class for the given testdoc\n",
        "    for c in C:\n",
        "        sum_[c] = logprior[c]\n",
        "        for word in testdoc.split():\n",
        "            if word in V:  #if present in vocab\n",
        "                sum_[c] += loglikelihood[V.index(word), c]\n",
        "\n",
        "    return np.argmax(sum_) #returns index of the class with the highest log probability (indicating predicted class for the testdoc)\n",
        "\n",
        "#list with the classes we have (aka labels)\n",
        "C = [0, 1, 2, 3, 4]\n",
        "\n",
        "#Trainingg\n",
        "logprior, loglikelihood, V = train_naive_bayes(train_df, C)\n",
        "\n",
        "#Testingg\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "for i in range(total):\n",
        "    predicted_label = test_naive_bayes(test_df.iloc[i]['sentence'], logprior, loglikelihood, C, V)\n",
        "    if predicted_label == test_df.iloc[i]['mapped_label']:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy when splitting sentence:\", accuracy)\n"
      ],
      "metadata": {
        "id": "4FMn6E9nb3bk",
        "outputId": "40011238-e20d-4463-cafe-414696743de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when splitting sentence: 0.39683257918552034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####same thing but with existing tokens in dataset not spliting the sentence"
      ],
      "metadata": {
        "id": "7XmaIeNrO4ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes1(D, C):\n",
        "    #calculate P(c) terms\n",
        "    Ndoc = len(D)\n",
        "    Nc = np.array([np.sum(D['mapped_label'] == c) for c in C])\n",
        "    logprior = np.log(Nc / Ndoc)\n",
        "\n",
        "    # building vocab with existing tokens in dataset\n",
        "    V = set()\n",
        "    for tokens in D['tokens']:\n",
        "        V.update(tokens)\n",
        "    V = list(V)\n",
        "\n",
        "    #calculate P(w|c) terms\n",
        "    bigdoc = {}\n",
        "    for c in C:\n",
        "        bigdoc[c] = [token for tokens in D[D['mapped_label'] == c]['tokens'] for token in tokens]\n",
        "\n",
        "    count = np.zeros((len(V), len(C)))\n",
        "    for i, word in enumerate(V):\n",
        "        for j, c in enumerate(C):\n",
        "            count[i, j] = bigdoc[c].count(word)\n",
        "\n",
        "    loglikelihood = np.log((count + 1) / (np.sum(count, axis=0, keepdims=True) + len(V)))\n",
        "\n",
        "    return logprior, loglikelihood, V\n",
        "\n",
        "def test_naive_bayes1(testdoc, logprior, loglikelihood, C, V):\n",
        "    sum_ = np.zeros(len(C))\n",
        "    for c in C:\n",
        "        sum_[c] = logprior[c]\n",
        "        for word in testdoc:\n",
        "            if word in V:\n",
        "                sum_[c] += loglikelihood[V.index(word), c]\n",
        "\n",
        "    return np.argmax(sum_)\n",
        "\n",
        "#classes\n",
        "C = [0, 1, 2, 3, 4]\n",
        "\n",
        "logprior, loglikelihood, V = train_naive_bayes1(train_df, C)\n",
        "\n",
        "correct = 0\n",
        "total = len(test_df)\n",
        "for i in range(total):\n",
        "    predicted_label = test_naive_bayes1(test_df.iloc[i]['tokens'], logprior, loglikelihood, C, V)\n",
        "    if predicted_label == test_df.iloc[i]['mapped_label']:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy with existing tokens in dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0clLFYYS99F-",
        "outputId": "6e6ad487-fe6a-4b71-d5af-65502b0bc4ed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with existing tokens in dataset: 0.2864253393665158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Metrics and comparison with sklearn"
      ],
      "metadata": {
        "id": "0qUnpJ67QAaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "#create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB(alpha=1.0))  # Alpha parameter for Laplace smoothing\n",
        "    ])\n",
        "\n",
        "#Train pipeline\n",
        "pipeline.fit(train_df['sentence'], train_df['mapped_label'])\n"
      ],
      "metadata": {
        "id": "sf6IBPCXvRuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "10620750-9588-4a21-ca21-6b0477588020"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
              "                ('classifier', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions_sklearn = pipeline.predict(test_df['sentence'])\n",
        "\n",
        "accuracy_sklearn = accuracy_score(test_df['mapped_label'], predictions_sklearn)\n",
        "print(\"Accuracy (scikit-learn):\", accuracy_sklearn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB57jC_bPsrJ",
        "outputId": "89f64db4-b092-408f-b316-9ef493ba2c66"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (scikit-learn): 0.4090497737556561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  function to make the Confusion Matrix\n",
        "def confusion_matrix_numpy(y_true, y_pred, num_classes):\n",
        "    confusion_mat = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        confusion_mat[true][pred] += 1\n",
        "    return confusion_mat\n",
        "\n",
        "conf_matrix_numpy = confusion_matrix_numpy(test_df['mapped_label'], predictions_sklearn, len(C))\n",
        "print(\"Confusion Matrix (only using numpy):\\n\", conf_matrix_numpy)\n",
        "\n",
        "# Precision, Recall, and F1 score per class\n",
        "def precision_recall_f1_numpy(conf_matrix):\n",
        "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
        "    recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "precision_numpy, recall_numpy, f1_numpy = precision_recall_f1_numpy(conf_matrix_numpy)\n",
        "print(\"Precision:\", precision_numpy)\n",
        "print(\"Recall:\", recall_numpy)\n",
        "print(\"F1 Score:\", f1_numpy)\n",
        "\n",
        "# Macro-averaged Precision, Recall, and F1 score\n",
        "def macro_averaged_metrics_numpy(precision, recall, f1):\n",
        "    macro_precision = np.mean(precision)\n",
        "    macro_recall = np.mean(recall)\n",
        "    macro_f1 = np.mean(f1)\n",
        "    return macro_precision, macro_recall, macro_f1\n",
        "\n",
        "precision_macro_numpy, recall_macro_numpy, f1_macro_numpy = macro_averaged_metrics_numpy(precision_numpy, recall_numpy, f1_numpy)\n",
        "print(\"Macro-averaged Precision:\", precision_macro_numpy)\n",
        "print(\"Macro-averaged Recall:\", recall_macro_numpy)\n",
        "print(\"Macro-averaged F1 Score:\", f1_macro_numpy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCgCXEZCSbMk",
        "outputId": "180789db-80a8-491b-9951-9332dcb20961"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (only using numpy):\n",
            " [[ 28 200  10  41   0]\n",
            " [ 22 415  57 127  12]\n",
            " [  8 154  34 181  12]\n",
            " [  2  88  39 340  41]\n",
            " [  2  39  12 259  87]]\n",
            "Precision: [0.4516129  0.46316964 0.22368421 0.35864979 0.57236842]\n",
            "Recall: [0.10035842 0.65560821 0.0874036  0.66666667 0.21804511]\n",
            "F1 Score: [0.16422287 0.54283846 0.12569316 0.46639232 0.31578947]\n",
            "Macro-averaged Precision: 0.41389699333828645\n",
            "Macro-averaged Recall: 0.3456164032418666\n",
            "Macro-averaged F1 Score: 0.3229872566299008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### better representation (for us)"
      ],
      "metadata": {
        "id": "3XGpXx7mQ4MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#create DataFrame for precision, recall, and F1 score\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_numpy,\n",
        "    'Recall': recall_numpy,\n",
        "    'F1 Score': f1_numpy\n",
        "}, index=C)  # Assuming C contains the class labels\n",
        "\n",
        "\n",
        "# create DataFrame for macro-averaged precision, recall, and F1 score\n",
        "macro_metrics_df = pd.DataFrame({\n",
        "    'Precision': [precision_macro_numpy],\n",
        "    'Recall': [recall_macro_numpy],\n",
        "    'F1 Score': [f1_macro_numpy]\n",
        "})\n",
        "\n",
        "print(\"Metrics per class (scikit-learn):\\n\", metrics_df)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Macro-averaged Metrics (scikit-learn):\\n\", macro_metrics_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHTx14dDP7NS",
        "outputId": "d3c4c01e-baaf-4309-f0dd-054b67aafdc4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics per class (scikit-learn):\n",
            "    Precision    Recall  F1 Score\n",
            "0   0.451613  0.100358  0.164223\n",
            "1   0.463170  0.655608  0.542838\n",
            "2   0.223684  0.087404  0.125693\n",
            "3   0.358650  0.666667  0.466392\n",
            "4   0.572368  0.218045  0.315789\n",
            "-------------------------------------\n",
            "Macro-averaged Metrics (scikit-learn):\n",
            "    Precision    Recall  F1 Score\n",
            "0   0.413897  0.345616  0.322987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression"
      ],
      "metadata": {
        "id": "WSRDo1llyxWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def features_extract(data):\n",
        "    # Initialize a set to store sentence and bigrams pairs\n",
        "    sentence_bigrams_set = set()\n",
        "    unique_bigrams = set()\n",
        "\n",
        "    # Extract relevant information from the dataset\n",
        "    for i in range(len(data)):\n",
        "        # Initialize a list to store unique word bi-grams\n",
        "        sentence = data['sentence'][i]\n",
        "        tokens = data['tokens'][i].split('|')\n",
        "\n",
        "        # Build bigrams\n",
        "        bigrams = [f\"{tokens[j]} {tokens[j + 1]}\" for j in range(len(tokens) - 1)]\n",
        "\n",
        "        # Add the pair to the set\n",
        "        sentence_bigrams_set.add((sentence, tuple(bigrams)))\n",
        "        unique_bigrams.update(bigrams)  # Updated this line\n",
        "\n",
        "    # Create a list of unique words\n",
        "    unique_words = sorted(list(set(word for sentence in data['tokens'] for word in sentence.split('|'))))\n",
        "\n",
        "    # Create a binary matrix to store bigram existence\n",
        "    bigram_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
        "\n",
        "    # Populate the matrix\n",
        "    for i, word1 in enumerate(unique_words):\n",
        "        for j, word2 in enumerate(unique_words):\n",
        "            # Convert (word1, word2) to a tuple for comparison\n",
        "            current_bigram = f\"{word1} {word2}\"\n",
        "            if current_bigram in unique_bigrams:\n",
        "                bigram_matrix[i, j] = 1\n",
        "\n",
        "    return bigram_matrix, unique_words\n",
        "\n",
        "\n",
        "train_dataset = sst_dataset['train']\n",
        "binary_bigram_matrix, unique_words = features_extract(train_dataset)\n",
        "\n",
        "# Print the resulting matrix\n",
        "print(\"Binary Bigram Matrix:\")\n",
        "print(binary_bigram_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1iehbsCzTtb",
        "outputId": "cc9954ba-ca73-45c3-8618-6a540036127b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Bigram Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'matrix' is your matrix\n",
        "matrix_sum = np.sum(binary_bigram_matrix)\n",
        "\n",
        "# Print the result\n",
        "print(\"Sum of all elements in the matrix:\", matrix_sum)\n"
      ],
      "metadata": {
        "id": "GqAu616a6rj1",
        "outputId": "15a76e4f-6609-4520-bfaa-eacee271f2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of all elements in the matrix: 87247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MultiClassLogisticRegression:\n",
        "\n",
        "    def __init__(self, n_iter=10000, thres=1e-3):\n",
        "        self.n_iter = n_iter\n",
        "        self.thres = thres\n",
        "        self.classes = ['0', '1', '2', '3', '4']\n",
        "        self.class_labels = [0, 1, 2, 3, 4]\n",
        "\n",
        "    def fit(self, X, y, batch_size=64, lr=0.001, rand_seed=4, verbose=False):\n",
        "        np.random.seed(rand_seed)\n",
        "        X = self.add_bias(X)\n",
        "        y = self.one_hot(y)\n",
        "        self.loss = []\n",
        "        self.weights = np.zeros(shape=(len(self.classes), X.shape[1]))\n",
        "        self.fit_data(X, y, batch_size, lr, verbose)\n",
        "        return self\n",
        "\n",
        "    def fit_data(self, X, y, batch_size, lr, verbose):\n",
        "        i = 0\n",
        "        while (not self.n_iter or i < self.n_iter):\n",
        "            self.loss.append(self.cross_entropy(y, self.predict_(X)))\n",
        "            idx = np.random.choice(X.shape[0], batch_size)\n",
        "            X_batch, y_batch = X[idx], y[idx]\n",
        "            error = y_batch - self.predict_(X_batch)\n",
        "            update = (lr * np.dot(error.T, X_batch))\n",
        "            self.weights += update\n",
        "            if np.abs(update).max() < self.thres:\n",
        "                break\n",
        "            if i % 1000 == 0 and verbose:\n",
        "                print('Training Accuracy at {} iterations is {}'.format(i, self.evaluate_(X, y)))\n",
        "            i += 1\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.predict_(self.add_bias(X))\n",
        "\n",
        "    def predict_(self, X):\n",
        "        pre_vals = np.dot(X, self.weights.T).reshape(-1, len(self.classes))\n",
        "        return self.softmax(pre_vals)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
        "\n",
        "    def predict_classes(self, X):\n",
        "        self.probs_ = self.predict(X)\n",
        "        return np.vectorize(lambda c: self.classes[c])(np.argmax(self.probs_, axis=1))\n",
        "\n",
        "    def add_bias(self, X):\n",
        "        return np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "    def one_hot(self, y):\n",
        "        return np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict_classes(X) == y)\n",
        "\n",
        "    def evaluate_(self, X, y):\n",
        "        return np.mean(np.argmax(self.predict_(X), axis=1) == np.argmax(y, axis=1))\n",
        "\n",
        "\n",
        "\n",
        "    def cross_entropy(self, y, probs):\n",
        "      exp_logits = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
        "      softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "      # Avoid numerical instability by adding a small epsilon\n",
        "      epsilon = 1e-15\n",
        "      softmax_probs = np.clip(softmax_probs, epsilon, 1 - epsilon)\n",
        "\n",
        "      # Calculate cross-entropy loss\n",
        "      loss = -np.sum(labels * np.log(softmax_probs)) / len(logits)\n",
        "\n",
        "      return loss\n",
        "\n",
        "# from sklearn import datasets\n",
        "# X,y = datasets.load_iris(return_X_y=True)\n",
        "# lr = MultiClassLogisticRegression(thres=1e-5)\n",
        "# lr.fit(X,y,lr=0.0001)\n",
        "# print(lr.score(X, y))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "multireg = MultiClassLogisticRegression(n_iter=5000)\n",
        "y = train_dataset['mapped_label']\n",
        "my_array = np.array(y)\n",
        "print(type(my_array))\n",
        "multireg.fit(binary_bigram_matrix,my_array , lr=0.01, batch_size=128, verbose=True)\n"
      ],
      "metadata": {
        "id": "_-U_DOUS63sc",
        "outputId": "732726cb-cc12-48ac-8ec5-4d7b62259d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (18280,5) (8544,5) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c7e778468f9c>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mmy_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmultireg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_bigram_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_array\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c7e778468f9c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, lr, rand_seed, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c7e778468f9c>\u001b[0m in \u001b[0;36mfit_data\u001b[0;34m(self, X, y, batch_size, lr, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c7e778468f9c>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(self, y, probs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;31m# Calculate cross-entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (18280,5) (8544,5) "
          ]
        }
      ]
    }
  ]
}